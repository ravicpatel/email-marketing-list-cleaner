{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import  re\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Project Description:\n",
    "This project addresses a common challenge faced by the client: managing user email data for marketing campaigns. \n",
    "The client had a dataset where a single table contained 10 columns of manually entered user email IDs. \n",
    "Due to multiple entries per user and the same email appearing across different columns, the client required a consolidated list of unique email addresses.\n",
    "\n",
    "The goal was to ensure that marketing emails are not sent multiple times to the same user, thereby optimizing email credits and improving campaign efficiency. Additionally, this unique email list could be used for targeted advertising on platforms like Google and Facebook, enhancing the reach and effectiveness of ads.\n",
    "\n",
    "To solve this, I developed a Python Streamlit application that allows the client to upload their dataset. \n",
    "The application processes the data and generates a single CSV file containing a consolidated, deduplicated list of unique email addresses. \n",
    "This output file enables the client to efficiently run email marketing campaigns and improve audience targeting through data integration with Google and Facebook Ads.\n",
    "Streamlit app is named as \"email_merge.py\"\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# For displaying output which can be scrolled if too big, if you wish you can remove this as my client had more than 70 rows of data i had to keep it.\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anish\\AppData\\Local\\Temp\\ipykernel_5468\\2099975121.py:1: DtypeWarning: Columns (25,26,54,55,57,59,62,65,68,69,72,76) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"data_file.csv\")\n"
     ]
    }
   ],
   "source": [
    "# Read the data file\n",
    "# Here your data can be in any form or file, if it is in sql, json, or any other format than logic to read differs\n",
    "# As my data file was of my client which i can not share because of legal restriction i can only share code by which i achieved given task\n",
    "# Hence data_file here consist of dummy data created from website: \"https://www.emailsverified.com/email-list-generator/\"\n",
    "df = pd.read_csv(\"data_file.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning email logic\n",
    "\n",
    "# Step 1: Clean email\n",
    "\n",
    "def clean_email_address(email):\n",
    "    # Define a regex for a valid email pattern\n",
    "    email_pattern = r'^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$'\n",
    "\n",
    "    # Check if the email matches the pattern, this will remove wrong email address entries from data\n",
    "    if re.match(email_pattern, str(email)):\n",
    "        return email\n",
    "    else:\n",
    "        return np.nan\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of the email columns to clean\n",
    "email_columns_to_clean = ['column_email_1', 'column_email_2', 'column_email_3', 'column_email_4', 'column_email_5', 'column_email_6', 'column_email_7', 'column_email_8', 'column_email_9', 'column_email_10']\n",
    "\n",
    "# Loop to clean email\n",
    "for col in email_columns_to_clean:\n",
    "    cleaned_col_name = col + '_cleaned'\n",
    "    df[cleaned_col_name] = df[col].apply(clean_email_address)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 filtering (optional), here i have filtered as per data and requirement, you have to filter as per your need or you can omit this step if you dont have any specific filter case which needs to be merged\n",
    "\n",
    "df_filtered_email = df[(df['column_filter_1'] == 'required_value_1') & (~df['column_filter_2'].isin(['required_value_2','required_value_3']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34656\n"
     ]
    }
   ],
   "source": [
    "# Step 3 Melting and merging in one columns\n",
    "\n",
    "df_melted_emails = df_filtered_email[[col + '_cleaned' for col in email_columns_to_clean]].melt(value_name = 'merged_email_cleaned').dropna(subset = ['merged_email_cleaned'])\n",
    "\n",
    "# Step 4 drop duplicates\n",
    "\n",
    "df_melted_emails = df_melted_emails[['merged_email_cleaned']].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "print(len(df_melted_emails))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporting merged email list into new csv file containing single column.\n",
    "\n",
    "df_melted_emails.to_csv('Merge_emails_2024_09_26.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mail_merge_python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
